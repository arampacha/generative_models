{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# default_exp vae"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "> And its variations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import typing\n",
    "from typing import Sequence, Union, Tuple\n",
    "\n",
    "from generative_models.layers import scale, unscale"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# export\n",
    "class VAEOutput(typing.NamedTuple):\n",
    "    pred:Tensor\n",
    "    kl_loss:Tensor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# export\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder:nn.Module, decoder:nn.Module, beta:float=1.):\n",
    "        super().__init__()\n",
    "        self.encoder, self.decoder = encoder, decoder\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x).chunk(2, -1)\n",
    "        z, kl_loss = self.reparametrize(mu, logvar)\n",
    "        out = self.decoder(z)\n",
    "        return VAEOutput(out, kl_loss)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        bs = mu.size(0)\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if self.training:\n",
    "            z = torch.randn_like(mu, requires_grad=False)*std + mu\n",
    "        else:\n",
    "            z = mu\n",
    "\n",
    "        kl_loss = self.beta * 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) / bs\n",
    "        return z, kl_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, z=None, n=100):\n",
    "        if z is None:\n",
    "            z = torch.randn(n, self.d_z, device=device)\n",
    "        return unscale(self.decoder(z)) \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct(self, x):\n",
    "        self.eval()\n",
    "        return unscale(self(x)[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vector Quantized VAE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# export\n",
    "from torch.autograd import Function"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#export\n",
    "class VQPseudoGrad(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z, q):\n",
    "        return q\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# export\n",
    "class VectorQuantizer(nn.Module):\n",
    "\n",
    "    def __init__(self, k:int, d:int, commitment_cost:float=0.25):\n",
    "        super().__init__()\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.embedding = nn.Parameter(torch.empty(k, d))\n",
    "        nn.init.uniform_(self.embedding, -1/k, 1/k)\n",
    "\n",
    "    def forward(self, z):\n",
    "        b,c,h,w = z.size()\n",
    "        z_ = z.permute(0,2,3,1)\n",
    "        e = self.embedding\n",
    "        distances = ((z_*z_).sum(-1, keepdim=True)\n",
    "                    -2*torch.einsum('...d, nd -> ...n', z_, e)\n",
    "                    +(e*e).sum(-1, keepdim=True).t())\n",
    "        code = distances.argmin(-1)\n",
    "        zq = F.embedding(code, e).permute(0,3,1,2).contiguous()\n",
    "        \n",
    "        e_latent_loss = F.mse_loss(zq.detach(), z)\n",
    "        q_latent_loss = F.mse_loss(zq, z.detach())\n",
    "        loss = q_latent_loss + e_latent_loss * self.commitment_cost\n",
    "        return VQPseudoGrad.apply(z, zq), loss, code\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return (f'(embedding): k={self.embedding.size(0)}, d={self.embedding.size(1)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class EMA(nn.Module):\n",
    "\n",
    "    def __init__(self, size:Tuple[int], gamma:float):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"avg\", torch.zeros(*size))\n",
    "        self.gamma = gamma\n",
    "        self.cor = 1\n",
    "\n",
    "    def update(self, val):\n",
    "        self.cor *= self.gamma\n",
    "        self.avg += (val - self.avg) * (1-self.gamma)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.avg / (1. - self.cor)\n",
    "\n",
    "    def updated_value(self, val):\n",
    "        self.update(val)\n",
    "        return self.value"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# hide\n",
    "ema = EMA((1, ), 0.9)\n",
    "\n",
    "for i in range(1,11):\n",
    "    print(ema.updated_value(i))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.])\n",
      "tensor([1.5263])\n",
      "tensor([2.0701])\n",
      "tensor([2.6313])\n",
      "tensor([3.2097])\n",
      "tensor([3.8052])\n",
      "tensor([4.4176])\n",
      "tensor([5.0466])\n",
      "tensor([5.6920])\n",
      "tensor([6.3534])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# export\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "\n",
    "    def __init__(self, k:int, d:int, commitment_cost:float=0.25, gamma=0.99, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.gamma, self.epsilon = gamma, epsilon\n",
    "        self.k = k\n",
    "        self.register_buffer(\"embedding\", (torch.empty(k, d)))\n",
    "        nn.init.uniform_(self.embedding, -1/k, 1/k)\n",
    "        self.ema_cluster_size = EMA((k, ), gamma=gamma)\n",
    "        self.ema_cluster_sum = EMA(self.embedding.size(), gamma=gamma)\n",
    "\n",
    "    def forward(self, z):\n",
    "        if z.dim() == 2:\n",
    "            nd = 1\n",
    "            b,c = z.size()\n",
    "            z_ = z\n",
    "        if z.dim() == 4:\n",
    "            nd = 2\n",
    "            b,c,h,w = z.size()\n",
    "            z_ = z.permute(0,2,3,1).view(-1, c)\n",
    "        e = self.embedding\n",
    "        distances = ((z_*z_).sum(-1, keepdim=True)\n",
    "                    -2*torch.einsum('...d, nd -> ...n', z_, e)\n",
    "                    +(e*e).sum(-1, keepdim=True).t())\n",
    "        code = distances.argmin(-1)\n",
    "        zq = F.embedding(code, e)\n",
    "        if nd == 2:\n",
    "            zq = zq.view(b,h,w,c).permute(0,3,1,2).contiguous()\n",
    "        \n",
    "        e_latent_loss = F.mse_loss(zq.detach(), z)\n",
    "        loss = e_latent_loss * self.commitment_cost\n",
    "\n",
    "        # EMA update for the codebook\n",
    "        if self.training:\n",
    "            code_oh = F.one_hot(code, self.k)\n",
    "            cluster_size = code_oh.sum(0)\n",
    "            upd_ema_cluster_size = self.ema_cluster_size.updated_value(cluster_size)\n",
    "            n = cluster_size.sum()\n",
    "            upd_ema_cluster_size = ((upd_ema_cluster_size + self.epsilon) /\n",
    "                                    (n + self.k * self.epsilon) * n)\n",
    "            cluster_sum = torch.zeros_like(self.embedding).scatter_add_(0, code.unsqueeze(-1).expand_as(z_), z_)\n",
    "            upd_ema_cluster_sum = self.ema_cluster_sum.updated_value(cluster_sum)\n",
    "            \n",
    "            self.embedding = upd_ema_cluster_sum / upd_ema_cluster_size[..., None]\n",
    "            \n",
    "        return VQPseudoGrad.apply(z, zq), loss, code\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return (f'(embedding): k={self.embedding.size(0)}, d={self.embedding.size(1)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# hide\n",
    "vectors = torch.randn(100, 8)\n",
    "vq = VectorQuantizerEMA(100, 8)\n",
    "for i in range(1000):\n",
    "    idx = torch.randint(0,100, (16,))\n",
    "    x = vectors[idx]\n",
    "    _, loss, _ = vq(x)\n",
    "    if (i+1)%50 == 0:\n",
    "        print(loss.item())\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.06493166089057922\n",
      "0.043664030730724335\n",
      "0.049629323184490204\n",
      "0.082007996737957\n",
      "0.0712009072303772\n",
      "0.06267855316400528\n",
      "0.06657259166240692\n",
      "0.0502491258084774\n",
      "0.06776320934295654\n",
      "0.06083505228161812\n",
      "0.06093211472034454\n",
      "0.07369934022426605\n",
      "0.06409207731485367\n",
      "0.05294540524482727\n",
      "0.06099759042263031\n",
      "0.07116104662418365\n",
      "0.05591275915503502\n",
      "0.07219719141721725\n",
      "0.07369670271873474\n",
      "0.05563058704137802\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class VQVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, k:int, d:int, commitment_cost:float=0.25, use_ema:bool=False):\n",
    "        super().__init__()\n",
    "        self.encoder, self.decoder = encoder, decoder\n",
    "        self.quantize = (VectorQuantizerEMA(k, d, commitment_cost) if use_ema else\n",
    "                         VectorQuantizer(k, d, commitment_cost))\n",
    "\n",
    "    def forward(self, x):\n",
    "        ze = self.encoder(x)\n",
    "        zq, vq_loss, code = self.quantize(ze)\n",
    "        x_hat = self.decoder(zq)\n",
    "        return x_hat, vq_loss, code\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, x):\n",
    "        ze = self.ecoder(x)\n",
    "        _, _, code = self.quantize(ze)\n",
    "        return code\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode(self, code):\n",
    "        zq = F.embedding(code, self.quantize.embedding)\n",
    "        if zq.dim() == 4:\n",
    "            zq = zq.permute(0,3,1,2).contiguous()\n",
    "        return self.decoder(zq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted 00_layers.ipynb.\n",
      "Converted 01_training.ipynb.\n",
      "Converted 02_made.ipynb.\n",
      "Converted 03_pixelcnn.ipynb.\n",
      "Converted 04_vae.ipynb.\n",
      "Converted 10_experiments.pixelcnn.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('torchenv': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "4af15c6377fd3f0f03723b0d6472f0c10dcd7b2afd49a75a2b51cefc0e0a1d19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}