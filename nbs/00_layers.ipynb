{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# default_exp layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Layers\n",
    "\n",
    "> Common layers, blocks and utils."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import Sequence, Union, Tuple"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#export\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "class Identity():\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "\n",
    "def exist(x):\n",
    "    return x is not None\n",
    "\n",
    "def ifnone(x, default):\n",
    "    return default if x is None else x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# export \n",
    "def scale(x):\n",
    "    return x*2 - 1\n",
    "\n",
    "def unscale(x):\n",
    "    return (x+1)/2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "def trainable_parameters(m:nn.Module):\n",
    "    return [p for p in m.parameters() if p.requires_grad]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# export\n",
    "class FullyConnected(nn.Sequential):\n",
    "\n",
    "    def __init__(self, d_in:int, d_out:int, bn=False, preact=False, activation=nn.ReLU) -> None:\n",
    "        layers = [activation()]\n",
    "        if bn: layers.insert(0, nn.BatchNorm1d(d_in if preact else d_out))\n",
    "        layers.insert(-1 if preact else 0, nn.Linear(d_in, d_out))\n",
    "        super().__init__(*layers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# export\n",
    "class MLP(nn.Sequential):\n",
    "    \"Multi-layer perceptron\"\n",
    "    def __init__(self, d_in:int, d_out:int, d_h:int, n_layers:int, hiddens:Sequence=None, bn:bool=False, preact:bool=False) -> None:\n",
    "        hiddens = ifnone(hiddens, [d_h]*n_layers)\n",
    "        ds = [d_in] + hiddens\n",
    "        layers = [FullyConnected(ds[i], ds[i+1], bn, preact) for i in range(len(ds)-1)]\n",
    "        layers.append(FullyConnected(hiddens[-1], d_out, bn, preact) if preact else nn.Linear(hiddens[-1], d_out))\n",
    "        super().__init__(*layers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "model = MLP(5, 10, 16, n_layers=3)\n",
    "x = torch.randn(4, 5)\n",
    "out = model(x)\n",
    "assert out.shape == (4, 10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "#export\n",
    "class Conv2dBlock(nn.Sequential):\n",
    "    \"Convolutional block. If preact is True will be BN-ACT-CONV as prposed in https://arxiv.org/abs/1603.05027\"\n",
    "    def __init__(self, c_in:int, c_out:int, ks:int, stride:int=1, padding:int=None, activation=nn.ReLU, preact=False):\n",
    "        padding = ifnone(padding, (ks-1)//2)\n",
    "        layers = [nn.BatchNorm2d(c_in if preact else c_out), activation(inplace=True)]\n",
    "        layers.insert(-1 if preact else 0, nn.Conv2d(c_in, c_out, ks, stride, padding))\n",
    "        super().__init__(*layers)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "bs, c_in, c_out, h, w = 4, 3, 8, 4, 4 \n",
    "conv = Conv2dBlock(c_in, c_out, 3, 2)\n",
    "x = torch.randn(bs, c_in, h, w)\n",
    "out = conv(x)\n",
    "assert out.shape == (bs, c_out, (h+1)//2, (w+1)//2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#hide\n",
    "bs, c_in, c_out, h, w = 4, 3, 8, 24, 24 \n",
    "conv = Conv2dBlock(c_in, c_out, 3, 2, preact=True)\n",
    "x = torch.randn(bs, c_in, h, w)\n",
    "out = conv(x)\n",
    "assert out.shape == (bs, c_out, (h+1)//2, (w+1)//2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#export\n",
    "class ConvTranspose2dBlock(nn.Sequential):\n",
    "    \"Convolutional block. If preact is True will be BN-ACT-CONV as prposed in https://arxiv.org/abs/1603.05027\"\n",
    "    def __init__(self, c_in:int, c_out:int, ks:int, stride:int=1, padding:int=None, activation=nn.ReLU, preact=False):\n",
    "        padding = ifnone(padding, (ks-1)//2)\n",
    "        layers = [nn.BatchNorm2d(c_in if preact else c_out), activation(inplace=True)]\n",
    "        layers.insert(-1 if preact else 0, nn.ConvTranspose2d(c_in, c_out, ks, stride, padding))\n",
    "        super().__init__(*layers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "bs, c_in, c_out, h, w = 4, 16, 8, 10, 10 \n",
    "conv = ConvTranspose2dBlock(c_in, c_out, 4, 2)\n",
    "x = torch.randn(bs, c_in, h, w)\n",
    "out = conv(x)\n",
    "assert out.shape == (bs, c_out, h*2, w*2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#export\n",
    "class ResBlock(nn.Module):\n",
    "    \"Convolutional block with skip connection\"\n",
    "    def __init__(self, c_in:int, c_out:int, ks:Union[int, Tuple], stride:int=1, padding:int=None, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        if isinstance(ks, int):\n",
    "            ks = (ks, ks)\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv2dBlock(c_in, c_out, ks[0], stride, padding, activation, preact=True),\n",
    "            Conv2dBlock(c_out, c_out, ks[1], 1, padding, activation, preact=True)\n",
    "        )\n",
    "\n",
    "        skip_layers = []\n",
    "        if stride != 1:\n",
    "            skip_layers.append(nn.MaxPool2d(stride, ceil_mode=True))\n",
    "        if c_in != c_out:\n",
    "            skip_layers.append(nn.Conv2d(c_in, c_out, 1))\n",
    "        self.skip = nn.Sequential(*skip_layers)\n",
    "\n",
    "        self.act = activation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.skip(x) + self.conv(x))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "bs, c_in, c_out, h, w = 4, 3, 8, 24, 24 \n",
    "conv = ResBlock(c_in, c_out, 3, 1)\n",
    "x = torch.randn(bs, c_in, h, w)\n",
    "out = conv(x)\n",
    "assert out.shape == (bs, c_out, h, w)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# hide\n",
    "bs, c_in, c_out, h, w = 4, 3, 8, 24, 24 \n",
    "conv = ResBlock(c_in, c_out, 3, 2)\n",
    "x = torch.randn(bs, c_in, h, w)\n",
    "out = conv(x)\n",
    "assert out.shape == (bs, c_out, (h+1)//2, (w+1)//2)\n",
    "conv = ResBlock(c_in, c_out, (3, 1))\n",
    "out = conv(x)\n",
    "assert out.shape == (bs, c_out, h, w)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/arto/anaconda3/envs/torchenv/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# export\n",
    "class ChanLayerNorm(nn.Module):\n",
    "    \"Channelwise LayerNorm\"\n",
    "    def __init__(self, d:int, **kwargs):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln(x.permute(0,2,3,1))\n",
    "        return x.permute(0,3,1,2).contiguous()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "x = torch.randn(1, 3, 2, 2)\n",
    "m = ChanLayerNorm(3)\n",
    "out = m(x)\n",
    "mu = out.mean(1)\n",
    "assert torch.allclose(mu+1, torch.ones_like(mu))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "#export\n",
    "class ConvNet(nn.Sequential):\n",
    "    \"Stack of Conv2dBlocks\"\n",
    "    def __init__(self, c_in:int, ks:int=3, n_layers=4, channels:Sequence=None, preact=False) -> None:\n",
    "        channels = ifnone(channels, [2**i for i in range(3, 3+n_layers)])\n",
    "        layers = [Conv2dBlock(c_in, channels[0], ks, 2, preact=preact)]\n",
    "        layers += [Conv2dBlock(channels[i], channels[i+1], ks, 2, preact=preact) for i in range(len(channels)-2)]\n",
    "        layers += [Conv2dBlock(channels[-2], channels[-1], ks, 2) if preact else nn.Conv2d(channels[-2], channels[-1], ks, 2, padding=(ks-1)//2)]\n",
    "        super().__init__(*layers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "model = ConvNet(1)\n",
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (0): Conv2dBlock(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (1): Conv2dBlock(\n",
       "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (2): Conv2dBlock(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            Conv2dBlock(c_in, 256, 4, 2),\n",
    "            nn.Conv2d(256, 256, 4, 2, 1),\n",
    "            ResBlock(256, 256, (3,1), 1, activation=Identity),\n",
    "            ResBlock(256, 256, (3,1), 1, activation=Identity)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted 00_layers.ipynb.\n",
      "Converted 01_training.ipynb.\n",
      "Converted 02_made.ipynb.\n",
      "Converted 03_pixelcnn.ipynb.\n",
      "Converted 10_experiments.pixelcnn.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('torchenv': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "4af15c6377fd3f0f03723b0d6472f0c10dcd7b2afd49a75a2b51cefc0e0a1d19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}