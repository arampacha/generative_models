# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_layers.ipynb (unless otherwise specified).

__all__ = ['identity', 'Identity', 'exist', 'ifnone', 'scale', 'unscale', 'trainable_parameters', 'FullyConnected',
           'MLP', 'Conv2dBlock', 'ConvTranspose2dBlock', 'ResBlock', 'ChanLayerNorm', 'ConvNet']

# Cell
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor

from typing import Sequence, Union, Tuple

# Cell
def identity(x):
    return x

class Identity():
    def __call__(self, x):
        return x

def exist(x):
    return x is not None

def ifnone(x, default):
    return default if x is None else x

# Cell
def scale(x):
    return x*2 - 1

def unscale(x):
    return (x+1)/2

# Cell
def trainable_parameters(m:nn.Module):
    return [p for p in m.parameters() if p.requires_grad]

# Cell
class FullyConnected(nn.Sequential):

    def __init__(self, d_in:int, d_out:int, bn=False, preact=False, activation=nn.ReLU) -> None:
        layers = [activation()]
        if bn: layers.insert(0, nn.BatchNorm1d(d_in if preact else d_out))
        layers.insert(-1 if preact else 0, nn.Linear(d_in, d_out))
        super().__init__(*layers)

# Cell
class MLP(nn.Sequential):
    "Multi-layer perceptron"
    def __init__(self, d_in:int, d_out:int, d_h:int, n_layers:int, hiddens:Sequence=None, bn:bool=False, preact:bool=False) -> None:
        hiddens = ifnone(hiddens, [d_h]*n_layers)
        ds = [d_in] + hiddens
        layers = [FullyConnected(ds[i], ds[i+1], bn, preact) for i in range(len(ds)-1)]
        layers.append(FullyConnected(hiddens[-1], d_out, bn, preact) if preact else nn.Linear(hiddens[-1], d_out))
        super().__init__(*layers)

# Cell
class Conv2dBlock(nn.Sequential):
    "Convolutional block. If preact is True will be BN-ACT-CONV as prposed in https://arxiv.org/abs/1603.05027"
    def __init__(self, c_in:int, c_out:int, ks:int, stride:int=1, padding:int=None, activation=nn.ReLU, preact=False):
        padding = ifnone(padding, (ks-1)//2)
        layers = [nn.BatchNorm2d(c_in if preact else c_out), activation(inplace=True)]
        layers.insert(-1 if preact else 0, nn.Conv2d(c_in, c_out, ks, stride, padding))
        super().__init__(*layers)



# Cell
class ConvTranspose2dBlock(nn.Sequential):
    "Convolutional block. If preact is True will be BN-ACT-CONV as prposed in https://arxiv.org/abs/1603.05027"
    def __init__(self, c_in:int, c_out:int, ks:int, stride:int=1, padding:int=None, activation=nn.ReLU, preact=False):
        padding = ifnone(padding, (ks-1)//2)
        layers = [nn.BatchNorm2d(c_in if preact else c_out), activation(inplace=True)]
        layers.insert(-1 if preact else 0, nn.ConvTranspose2d(c_in, c_out, ks, stride, padding))
        super().__init__(*layers)

# Cell
class ResBlock(nn.Module):
    "Convolutional block with skip connection"
    def __init__(self, c_in:int, c_out:int, ks:Union[int, Tuple], stride:int=1, padding:int=None, activation=nn.ReLU):
        super().__init__()
        if isinstance(ks, int):
            ks = (ks, ks)
        self.conv = nn.Sequential(
            Conv2dBlock(c_in, c_out, ks[0], stride, padding, activation, preact=True),
            Conv2dBlock(c_out, c_out, ks[1], 1, padding, activation, preact=True)
        )

        skip_layers = []
        if stride != 1:
            skip_layers.append(nn.MaxPool2d(stride, ceil_mode=True))
        if c_in != c_out:
            skip_layers.append(nn.Conv2d(c_in, c_out, 1))
        self.skip = nn.Sequential(*skip_layers)

        self.act = activation()

    def forward(self, x):
        return self.act(self.skip(x) + self.conv(x))


# Cell
class ChanLayerNorm(nn.Module):
    "Channelwise LayerNorm"
    def __init__(self, d:int, **kwargs):
        super().__init__()
        self.ln = nn.LayerNorm(d, **kwargs)

    def forward(self, x):
        x = self.ln(x.permute(0,2,3,1))
        return x.permute(0,3,1,2).contiguous()

# Cell
class ConvNet(nn.Sequential):
    "Stack of Conv2dBlocks"
    def __init__(self, c_in:int, ks:int=3, n_layers=4, channels:Sequence=None, preact=False) -> None:
        channels = ifnone(channels, [2**i for i in range(3, 3+n_layers)])
        layers = [Conv2dBlock(c_in, channels[0], ks, 2, preact=preact)]
        layers += [Conv2dBlock(channels[i], channels[i+1], ks, 2, preact=preact) for i in range(len(channels)-2)]
        layers += [Conv2dBlock(channels[-2], channels[-1], ks, 2) if preact else nn.Conv2d(channels[-2], channels[-1], ks, 2, padding=(ks-1)//2)]
        super().__init__(*layers)