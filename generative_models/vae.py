# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_vae.ipynb (unless otherwise specified).

__all__ = ['VAEOutput', 'VAE', 'VQPseudoGrad', 'VectorQuantizer', 'VectorQuantizerEMA']

# Cell
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor

import typing
from typing import Sequence, Union, Tuple

from .layers import scale, unscale

# Cell
class VAEOutput(typing.NamedTuple):
    pred:Tensor
    kl_loss:Tensor

# Cell
class VAE(nn.Module):

    def __init__(self, encoder:nn.Module, decoder:nn.Module, beta:float=1.):
        super().__init__()
        self.encoder, self.decoder = encoder, decoder
        self.beta = beta

    def forward(self, x):
        mu, logvar = self.encoder(x).chunk(2, -1)
        z, kl_loss = self.reparametrize(mu, logvar)
        out = self.decoder(z)
        return VAEOutput(out, kl_loss)

    def reparametrize(self, mu, logvar):
        bs = mu.size(0)
        std = logvar.mul(0.5).exp_()
        if self.training:
            z = torch.randn_like(mu, requires_grad=False)*std + mu
        else:
            z = mu

        kl_loss = self.beta * 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)) / bs
        return z, kl_loss

    @torch.no_grad()
    def sample(self, z=None, n=100):
        if z is None:
            z = torch.randn(n, self.d_z, device=device)
        return unscale(self.decoder(z))

    @torch.no_grad()
    def reconstruct(self, x):
        self.eval()
        return unscale(self(x)[0])

# Cell
from torch.autograd import Function

# Cell
class VQPseudoGrad(Function):
    @staticmethod
    def forward(ctx, z, q):
        return q
    @staticmethod
    def backward(ctx, grad_output):
        return grad_output, None


# Cell
class VectorQuantizer(nn.Module):

    def __init__(self, k:int, d:int, commitment_cost:float=0.25):
        super().__init__()
        self.commitment_cost = commitment_cost
        self.embedding = nn.Parameter(torch.empty(k, d))
        nn.init.uniform_(self.embedding, -1/k, 1/k)

    def forward(self, z):
        b,c,h,w = z.size()
        z_ = z.permute(0,2,3,1)
        e = self.embedding
        distances = ((z_*z_).sum(-1, keepdim=True)
                    -2*torch.einsum('...d, nd -> ...n', z_, e)
                    +(e*e).sum(-1, keepdim=True).t())
        code = distances.argmin(-1)
        zq = F.embedding(code, e).permute(0,3,1,2).contiguous()

        e_latent_loss = F.mse_loss(zq.detach(), z)
        q_latent_loss = F.mse_loss(zq, z.detach())
        loss = q_latent_loss + e_latent_loss * self.commitment_cost
        return VQPseudoGrad.apply(z, zq), loss, code

    def extra_repr(self):
        return (f'(embedding): k={self.embedding.size(0)}, d={self.embedding.size(1)}')

# Cell
class VectorQuantizerEMA(nn.Module):

    def __init__(self, k:int, d:int, commitment_cost:float=0.25, gamma=0.99, epsilon=1e-5):
        super().__init__()
        self.commitment_cost = commitment_cost
        self.gamma, self.epsilon = gamma, epsilon
        self.k = k
        self.register_buffer("embedding", (torch.empty(k, d)))
        nn.init.uniform_(self.embedding, -1/k, 1/k)
        self.ema_cluster_size = EMA((k, ), gamma=gamma)
        self.ema_cluster_sum = EMA(self.embedding.size(), gamma=gamma)

    def forward(self, z):
        if z.dim() == 2:
            nd = 1
            b,c = z.size()
            z_ = z
        if z.dim() == 4:
            nd = 2
            b,c,h,w = z.size()
            z_ = z.permute(0,2,3,1).view(-1, c)
        e = self.embedding
        distances = ((z_*z_).sum(-1, keepdim=True)
                    -2*torch.einsum('...d, nd -> ...n', z_, e)
                    +(e*e).sum(-1, keepdim=True).t())
        code = distances.argmin(-1)
        zq = F.embedding(code, e)
        if nd == 2:
            zq = zq.view(b,h,w,c).permute(0,3,1,2).contiguous()

        e_latent_loss = F.mse_loss(zq.detach(), z)
        loss = e_latent_loss * self.commitment_cost

        # EMA update for the codebook
        if self.training:
            code_oh = F.one_hot(code, self.k)
            cluster_size = code_oh.sum(0)
            upd_ema_cluster_size = self.ema_cluster_size.updated_value(cluster_size)
            n = cluster_size.sum()
            upd_ema_cluster_size = ((upd_ema_cluster_size + self.epsilon) /
                                    (n + self.k * self.epsilon) * n)
            cluster_sum = torch.zeros_like(self.embedding).scatter_add_(0, code.unsqueeze(-1).expand_as(z_), z_)
            upd_ema_cluster_sum = self.ema_cluster_sum.updated_value(cluster_sum)

            self.embedding = upd_ema_cluster_sum / upd_ema_cluster_size[..., None]

        return VQPseudoGrad.apply(z, zq), loss, code

    def extra_repr(self):
        return (f'(embedding): k={self.embedding.size(0)}, d={self.embedding.size(1)}')