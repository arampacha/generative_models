# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_made.ipynb (unless otherwise specified).

__all__ = ['make_masks', 'MaskedLinear', 'SimpleMADE', 'MADE']

# Cell
import torch
from torch import nn
import torch.nn.functional as F

from typing import Sequence

# Cell
def make_masks(d:int, ks:Sequence):
    L = len(ks)
    ms = [torch.arange(d)]
    masks = []
    for l, k in enumerate(ks):
        m = torch.randint(min(ms[l]), d-1, (k,))
        ms.append(m)
        mask = torch.where(ms[l+1][..., None] >= ms[l][None], 1., 0.)
        masks.append(mask)
    mask = torch.where(ms[0][..., None] > ms[-1][None], 1., 0.)
    masks.append(mask)
    return masks


# Cell
class MaskedLinear(nn.Module):
    "Masked linear layer"
    def __init__(self, d_in:int, d_out:int, mask=None):
        super().__init__()
        self.w = nn.Parameter(torch.randn(d_out, d_in))
        self.b = nn.Parameter(torch.zeros(d_out))
        if mask is None:
            mask = torch.ones(d_out, d_in)
        else:
            assert mask.size() == self.w.size()
        self.register_buffer("mask", mask)
        torch.nn.init.kaiming_normal_(self.w)

    def forward(self, x, mask=None):
        if mask is not None:
            return F.linear(x, self.w*mask, self.b)
        elif self.mask is not None:
            return F.linear(x, self.w*self.mask, self.b)
        else:
            print("Using `MaskedLinear` but no mask is provided. Layer acts like nn.Linear")
            return F.linear(x, self.w, self.b)

    def set_mask(self, mask:torch.Tensor):
        assert mask.size() == self.w.size()
        self.mask = mask


# Cell
class SimpleMADE(nn.Module):

    def __init__(self, d_in:int, d_h:int, n_layers:int, add_direct=True):
        super().__init__()
        dims = [d_in] + [d_h]*n_layers + [d_in]
        masks = make_masks(d_in, dims[1:-1])
        layers = [MaskedLinear(dims[i], dims[i+1], masks[i]) for i in range(len(masks))]
        self.net = nn.Sequential(*layers)
        if add_direct:
            self.direct = MaskedLinear(d_in, d_in, torch.tril(torch.ones(d_in, d_in), diagonal=-1))
        else:
            self.direct = None

    def forward(self, x):
        out = self.net(x)
        if self.direct is not None:
            out += self.direct(x)
        return out



# Cell
class MADE(nn.Module):

    def __init__(self, d_in:int, d_h:int, n_layers:int, add_direct=True, outs_per_input=1):
        super().__init__()
        self.outs_per_input = outs_per_input
        dims = [d_in] + [d_h]*n_layers + [d_in*outs_per_input]
        self.masks = make_masks(d_in, dims[1:-1])
        layers = [MaskedLinear(dims[i], dims[i+1], self.masks[i]) for i in range(len(self.masks)-1)]
        layers += [MaskedLinear(dims[-2], dims[-1])]
        self.layers = nn.ModuleList(layers)
        if add_direct:
            direct_mask = torch.tril(torch.ones(d_in, d_in), diagonal=-1).repeat(outs_per_input,1)
            self.direct = MaskedLinear(d_in, d_in*outs_per_input, direct_mask)
        else:
            self.direct = None

    def forward(self, x):
        out = x
        for layer in self.layers[:-1]:
            out = layer(out)
        out = self.layers[-1](out, mask=self.masks[-1].repeat(self.outs_per_input,1))

        if self.direct is not None:
            out += self.direct(x)
        return out
